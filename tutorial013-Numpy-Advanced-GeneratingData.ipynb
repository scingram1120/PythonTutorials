{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38d572b",
   "metadata": {},
   "source": [
    "## Advanced programming III\n",
    "### Understanding and creating correlated datasets and how to create functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809aa3e2",
   "metadata": {},
   "source": [
    "As we said before, the core of data science is computer programming. \n",
    "\n",
    "To really explore data, we need to be able to write code to \n",
    "\n",
    "  (1) wrangle or even generate data that has the properties needed for analysis and \n",
    "  \n",
    "  (2) do actual data analysis and visualization.\n",
    "\n",
    "If data science didn't involve programming – if it only involved clicking buttons in a statistics program like SPSS – it wouldn't be called data *science*. In fact, it wouldn't even be a \"thing\" at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30775168",
   "metadata": {},
   "source": [
    "### Learning goals:\n",
    " \n",
    " - Understand how to generate correlated variables.\n",
    " \n",
    " - More indexing\n",
    " \n",
    " - More experiments with loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc85df",
   "metadata": {},
   "source": [
    "#### Generate correlated datasets\n",
    "\n",
    "In thispart of the tutorial we will learn how generate datasets that are 'related.' While doing that we will practice a few things learned recently in previous tutorials:\n",
    "\n",
    "  - Plotting with matplotlib\n",
    "  - generating numpy arrays\n",
    "  - indexing into arrays\n",
    "  - using `while` loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d921de",
   "metadata": {},
   "source": [
    "First thing first, we will import the basic libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ca534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae26e00",
   "metadata": {},
   "source": [
    "After that we will create a few datasets. More specifically, we will create `n` datasets each called `x` (say 5 datasets, where `n=5`). Each dataset will have the length of `m` (where for example, `m` could be 100), this means for example that each dataset will have the shape of (m,1) or in our example (100,1).\n",
    "\n",
    "After that, we will create another group of `n` datasets called `y` of the same shape of `x`. Each one of the `y` datasets will have a corresponding `x` dataset that it will be correlated with.\n",
    "\n",
    "This means that for each dataset in `x` there will be a dataset in `y` that is correlated with it.\n",
    "\n",
    "Let's get started with a hands on method. Forst we will make the example of a single dataset `x` and a correlated dataset `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90569e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first build the dataset `x`, \n",
    "# we will use our standard method\n",
    "# based on randn\n",
    "m  = 1000\n",
    "mu = 5\n",
    "sd = 1\n",
    "x  = mu + sd*np.random.randn(m,1)\n",
    "\n",
    "# let take a look at it\n",
    "plt.hist(x, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee412a",
   "metadata": {},
   "source": [
    "OK. After generating the first dataset we will generate a second dataset, let's call it `y`. This second dataset will be correlated to the first.\n",
    "\n",
    "To generate a dataset correlated to `x` we will indeed use `x` as our base for the data and add on top of `x` a small amount of noise, let's call it `noise`. `noise` represents the small (or larger) difference between `x` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.random.randn(m,1)\n",
    "y = x + err\n",
    "plt.hist(y,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035d734",
   "metadata": {},
   "source": [
    "OK. The two histograms seem similar (similar range and height), but it is difficult to judge if `x` and `y` are indeed correlated. To do that we need to make a scatter plot.\n",
    "\n",
    "`matplotlib` has a convenient function for scatter plots, `plt.scatter()`, we will use that function to take a look at whether the two datasets are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aac1a9",
   "metadata": {},
   "source": [
    "Great, the symbols should be aligned along the major diagonal. This means that they are indeed correlated. To get to understand more what we did above, let's think about `err`.\n",
    "\n",
    "Imagine, if there were no error, e.g., no `err`. That would mean that there would be no difference between `x` and `y`. Literally, the two datasets would be identical.\n",
    "\n",
    "We can do that with the code above by setting `err` to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbe9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 0\n",
    "y = x + err\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932919e2",
   "metadata": {},
   "source": [
    "The symbols should all lay on the major diagonal. So, `err` effectively controls the level of correlation between `x` and `y`. So if we set it to something small, in other words if we add only a small amount of error then the two arrays (`x` and `y`) would be very similar. For example, let's try setting it up to 10% of the original `err`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ce0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.random.randn(m,1);\n",
    "err = err*0.1 # 0.1 -> scaling factor\n",
    "y = x + err\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c6e9b",
   "metadata": {},
   "source": [
    "OK. It should have worked. The error added is not large, the symbols should lay almost on the diagonal, but not quite.\n",
    "\n",
    "As we increase the `err` the symbols should move away from the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.random.randn(m,1);\n",
    "scaling_factor = 0.9\n",
    "err = err*scaling_factor \n",
    "y = x + err\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c2e2c",
   "metadata": {},
   "source": [
    "One way to think about the scaling factor and `err` is that they are related to correlation. Indeed, they are not directly related to correlation (not a one-to-one relationship, but a proxy). \n",
    "\n",
    "The scaling factor is inversely related to correlation because as the scaling factor increases the correlation decreases. Furthermore, they are not directly related to correlation because they both depend on a couple of variables, for example, the variance of the distributions (both `err` and `x` will affect the relationship between the correlation and the scaling factor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d49cd6",
   "metadata": {},
   "source": [
    "Python has a method to generate couples of correlated arrays. We will now briefly explore it, but leave a deeper dive on each function to you. You are suggested to further explore the code below and its implications. It might come helpful to us later down the road, you never know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d7803",
   "metadata": {},
   "source": [
    "#### A more principled way to make correlated datasets\n",
    "\n",
    "NumPy has a function called `multivariate_normal` that generates pairs of correlated datasets. The correlation values can be specified conveniently. A little bit of thinking is required, though. The function uses the covariance matrix. The covariance matrix is composed of 4 numbers. Two of the numbers describe the variances of the two datasamples we want to generate. The other two values describe the correlation between the samples and are generally called `covariances` (co-variations or co-relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa555a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  numpy.random import multivariate_normal # we import the function\n",
    "x_mu = 0; # we set up the mean of the first set of data points \n",
    "y_mu = 0; # we set up the mean of the second sample\n",
    "x_var = 1; # the variance of the first sample\n",
    "y_var = 1; # the variance of the second sample\n",
    "cov = 0.9; # this is the covariance (can be thought of as correlation)\n",
    "\n",
    "# the function multivariate_normal will need a matrix to control\n",
    "# the relation between the samples, this matrix is called covariance matrix\n",
    "cov_m = [[x_var, cov],\n",
    "         [cov, y_var]]\n",
    "\n",
    "# we now create the two data sets by setting the the proper\n",
    "# means and passing the covariance matrix, we also pass the\n",
    "# requested size of the sample\n",
    "data = multivariate_normal([x_mu, y_mu], cov_m, size=1000)\n",
    "\n",
    "# We can plot the two data sets\n",
    "x, y = data[:,0], data[:,1]\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec207a",
   "metadata": {},
   "source": [
    "#### Creating many correlated datasets\n",
    "\n",
    "Imagine now if we were asked to create a series of correlated datasets. Not one, nottwo, more than that.\n",
    "\n",
    "Once the basic code used to build one is known. The rest of the datasets can be generated reusing the same code and putting the code inside a loop. Below we will show how to create 5 datasets using a `while` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c20772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0;\n",
    "n_datasets = 5;\n",
    "siz_datasets = 1000;\n",
    "\n",
    "x_mu = 1;  # mean of the first dataset \n",
    "y_mu = 1;  # mean of the second dataset\n",
    "x_var = 2; # the variance of the first dataset\n",
    "y_var = 2; # the variance of the second dataset\n",
    "cov = 0.85; # this is the covariance (can be thought of as correlation)\n",
    "\n",
    "# covariance matrix\n",
    "cov_m = [[x_var, cov],\n",
    "         [cov, y_var]]\n",
    "\n",
    "while counter < n_datasets :\n",
    "    data = multivariate_normal([x_mu, y_mu], \n",
    "                               cov_m, \n",
    "                               size=siz_datasets)\n",
    "    x, y = data[:,0], data[:,1]\n",
    "    counter = counter + 1\n",
    "\n",
    "    # Make a plot, show it, wait some time\n",
    "    print(\"Plotting dataset: \", counter)\n",
    "    plt.scatter(x, y);\n",
    "    plt.show() ;\n",
    "    plt.pause(0.05)\n",
    "\n",
    "else:\n",
    "    print(\"DONE Plotting datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aee6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
